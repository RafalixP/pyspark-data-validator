"""
PySpark Concepts - Nauka bez uruchamiania Spark
Pokazuje rÃ³Å¼nice miÄ™dzy pandas a PySpark na przykÅ‚adach kodu
"""

def show_pandas_vs_pyspark_examples():
    """PorÃ³wnanie skÅ‚adni pandas vs PySpark bez uruchamiania."""
    
    print("=== PANDAS VS PYSPARK - PORÃ“WNANIE SKÅADNI ===\n")
    
    # 1. Podstawowe operacje
    print("1. PODSTAWOWE OPERACJE")
    print("PANDAS:")
    print("  df = pd.read_csv('data.csv')")
    print("  df.head()")
    print("  df['column'].sum()")
    print()
    print("PYSPARK:")
    print("  df = spark.read.csv('data.csv', header=True)")
    print("  df.show(5)")
    print("  df.agg(sum('column')).collect()")
    print()
    
    # 2. Filtrowanie
    print("2. FILTROWANIE")
    print("PANDAS:")
    print("  df[df['age'] > 25]")
    print("  df.query('age > 25 and salary > 5000')")
    print()
    print("PYSPARK:")
    print("  df.filter(col('age') > 25)")
    print("  df.filter((col('age') > 25) & (col('salary') > 5000))")
    print()
    
    # 3. Grupowanie
    print("3. GRUPOWANIE")
    print("PANDAS:")
    print("  df.groupby('department').agg({'salary': 'mean'})")
    print()
    print("PYSPARK:")
    print("  df.groupBy('department').agg(avg('salary'))")
    print()
    
    # 4. Nowe kolumny
    print("4. NOWE KOLUMNY")
    print("PANDAS:")
    print("  df['bonus'] = df['salary'] * 0.1")
    print("  df['category'] = df['salary'].apply(lambda x: 'High' if x > 5000 else 'Low')")
    print()
    print("PYSPARK:")
    print("  df.withColumn('bonus', col('salary') * 0.1)")
    print("  df.withColumn('category', when(col('salary') > 5000, 'High').otherwise('Low'))")
    print()
    
    # 5. JOIN
    print("5. JOIN")
    print("PANDAS:")
    print("  pd.merge(df1, df2, on='id', how='inner')")
    print()
    print("PYSPARK:")
    print("  df1.join(df2, df1.id == df2.id, 'inner')")
    print()
    
    # 6. SQL
    print("6. SQL")
    print("PANDAS:")
    print("  # Brak natywnego wsparcia SQL")
    print()
    print("PYSPARK:")
    print("  df.createOrReplaceTempView('employees')")
    print("  spark.sql('SELECT * FROM employees WHERE salary > 5000')")
    print()

def show_key_differences():
    """Pokazuje kluczowe rÃ³Å¼nice miÄ™dzy pandas a PySpark."""
    
    print("=== KLUCZOWE RÃ“Å»NICE ===\n")
    
    differences = [
        ("Wykonanie", "pandas: Eager (natychmiastowe)", "PySpark: Lazy (opÃ³Åºnione)"),
        ("PamiÄ™Ä‡", "pandas: Wszystko w RAM", "PySpark: Disk + Memory"),
        ("Skala", "pandas: Do ~10GB", "PySpark: Terabajty+"),
        ("RÃ³wnolegÅ‚oÅ›Ä‡", "pandas: Ograniczona", "PySpark: PeÅ‚na paralelizacja"),
        ("SQL", "pandas: Brak", "PySpark: Natywne wsparcie"),
        ("BÅ‚Ä™dy", "pandas: Natychmiast", "PySpark: Przy wykonaniu (.show(), .collect())"),
        ("Optymalizacja", "pandas: Manualna", "PySpark: Automatyczna (Catalyst)")
    ]
    
    for aspect, pandas_way, pyspark_way in differences:
        print(f"{aspect}:")
        print(f"  ğŸ“Š {pandas_way}")
        print(f"  âš¡ {pyspark_way}")
        print()

def show_when_to_use():
    """Pokazuje kiedy uÅ¼ywaÄ‡ pandas vs PySpark."""
    
    print("=== KIEDY UÅ»YWAÄ† KTÃ“REGO? ===\n")
    
    print("ğŸ¼ PANDAS - gdy:")
    print("  â€¢ Dane < 5GB")
    print("  â€¢ Szybkie prototypowanie")
    print("  â€¢ Analiza eksploracyjna")
    print("  â€¢ Praca lokalna")
    print("  â€¢ Potrzebujesz wszystkich funkcji pandas")
    print()
    
    print("âš¡ PYSPARK - gdy:")
    print("  â€¢ Dane > 10GB")
    print("  â€¢ Produkcja/pipeline'y")
    print("  â€¢ Klaster Spark dostÄ™pny")
    print("  â€¢ Integracja z big data ecosystem")
    print("  â€¢ Potrzebujesz skalowalnoÅ›ci")
    print()

def show_learning_path():
    """Pokazuje Å›cieÅ¼kÄ™ nauki PySpark dla uÅ¼ytkownikÃ³w pandas."""
    
    print("=== ÅšCIEÅ»KA NAUKI PANDAS â†’ PYSPARK ===\n")
    
    steps = [
        "1. Zrozum lazy evaluation - operacje budujÄ… plan, nie wykonujÄ… siÄ™ od razu",
        "2. Naucz siÄ™ .show() i .collect() - to wywoÅ‚uje wykonanie",
        "3. Opanuj DataFrame API - podobny do pandas, ale z col() i when()",
        "4. Poznaj Spark SQL - moÅ¼esz uÅ¼ywaÄ‡ znanego SQL",
        "5. Zrozum partycjonowanie - jak dane sÄ… dzielone",
        "6. Naucz siÄ™ cache() - przechowywanie w pamiÄ™ci",
        "7. Opanuj window functions - analiza w oknach",
        "8. Poznaj broadcast joins - optymalizacja JOIN'Ã³w"
    ]
    
    for step in steps:
        print(f"  {step}")
    print()

def main():
    """GÅ‚Ã³wna funkcja demonstracyjna."""
    show_pandas_vs_pyspark_examples()
    show_key_differences()
    show_when_to_use()
    show_learning_path()
    
    print("=== NASTÄ˜PNE KROKI ===")
    print("1. Zainstaluj Java (OpenJDK 11 lub 17)")
    print("2. Uruchom sql_to_pyspark_guide.py")
    print("3. Eksperymentuj z example_usage.py")
    print("4. RozwiÄ…Å¼ Ä‡wiczenia w sql_exercises.py")

if __name__ == "__main__":
    main()