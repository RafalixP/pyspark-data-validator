# PySpark Data Quality Validator

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5.0-orange)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Projekt demonstrujÄ…cy walidacjÄ™ jakoÅ›ci danych w PySpark - od podstawowych funkcji pandas do zaawansowanych pipeline'Ã³w big data.

## ğŸ¯ Cel projektu

Przygotowanie do pozycji **Test Engineer â€“ Data & AI** poprzez praktycznÄ… naukÄ™:
- Przepisywania funkcji pandas na PySpark
- Testowania pipeline'Ã³w danych
- Optymalizacji wydajnoÅ›ci big data
- Implementacji CI/CD dla jakoÅ›ci danych

## Struktura projektu

```
pyspark-data-validator/
â”œâ”€â”€ requirements.txt          # ZaleÅ¼noÅ›ci
â”œâ”€â”€ pyspark_validator.py      # GÅ‚Ã³wna klasa walidatora
â”œâ”€â”€ example_usage.py          # Podstawowe przykÅ‚ady uÅ¼ycia
â”œâ”€â”€ advanced_examples.py      # Zaawansowane scenariusze
â”œâ”€â”€ test_pyspark_validator.py # Testy jednostkowe
â””â”€â”€ README.md                # Ten plik
```

## Instalacja

```bash
pip install -r requirements.txt
```

## Uruchomienie

### Podstawowe przykÅ‚ady
```bash
python example_usage.py
```

### Zaawansowane funkcje
```bash
python advanced_examples.py
```

### Testy
```bash
pytest test_pyspark_validator.py -v
```

## Kluczowe funkcjonalnoÅ›ci

### 1. Walidacja jakoÅ›ci danych
- Sprawdzanie duplikatÃ³w
- Wykrywanie brakujÄ…cych wartoÅ›ci
- Walidacja formatÃ³w (email, telefon)
- Sprawdzanie ujemnych wartoÅ›ci
- ZgodnoÅ›Ä‡ schematu

### 2. Zaawansowane funkcje
- Pipeline walidacji jakoÅ›ci
- Wykrywanie anomalii (IQR)
- Åšledzenie pochodzenia danych
- Analiza w oknach przesuwnych
- Strategie prÃ³bkowania

### 3. Optymalizacja wydajnoÅ›ci
- Partycjonowanie danych
- Cache'owanie DataFrame
- Monitorowanie wykonania

## PrzykÅ‚ad uÅ¼ycia

```python
from pyspark_validator import PySparkDataValidator

# Inicjalizacja
validator = PySparkDataValidator()

# Sprawdzenie duplikatÃ³w
result = validator.check_duplicates(df, columns=["id"])
print(f"Duplikaty: {result['duplicate_count']}")

# Walidacja emaili
email_result = validator.validate_email_column(df, "email")
print(f"Poprawne emaile: {email_result['validation_rate']}%")

# ZamkniÄ™cie
validator.close()
```

## Scenariusze testowe dla rozmowy

1. **Big Data Processing**: Jak obsÅ‚uÅ¼yÄ‡ DataFrame z milionami rekordÃ³w?
2. **Performance Tuning**: Optymalizacja zapytaÅ„ i partycjonowania
3. **Data Quality Pipelines**: Automatyzacja walidacji w CI/CD
4. **Schema Evolution**: ObsÅ‚uga zmian w schemacie danych
5. **Anomaly Detection**: Wykrywanie nietypowych wzorcÃ³w w danych

## Kluczowe rÃ³Å¼nice PySpark vs Pandas

| Aspekt | Pandas | PySpark |
|--------|--------|---------|
| Przetwarzanie | Single-machine | Distributed |
| Lazy Evaluation | Nie | Tak |
| Memory Management | In-memory | Disk + Memory |
| SQL Support | Ograniczone | PeÅ‚ne wsparcie |
| Scalability | Ograniczona | Nieograniczona |

## ğŸ¤ Contributing

ChÄ™tnie przyjmÄ™ pull requesty! SzczegÃ³lnie mile widziane:
- Nowe funkcje walidacyjne
- Optymalizacje wydajnoÅ›ci
- Dodatkowe testy
- PrzykÅ‚ady integracji z Azure/AWS

## ğŸ“§ Kontakt

RafaÅ‚ Pieczka - [LinkedIn](https://linkedin.com/in/rafal-pieczka)

## ğŸ“„ Licencja

Projekt jest dostÄ™pny na licencji MIT - zobacz [LICENSE](LICENSE) dla szczegÃ³Å‚Ã³w.